# REQ-002: Integración de backend vLLM custom para LLM + Embeddings

## STATUS: IMPLEMENTED
**Created**: 2026-02-17 | **Implemented**: 2026-02-17 | **Context**: AI Provider Integration (Inference + Embeddings)

---

## PROBLEM
Archon soporta múltiples providers (OpenAI, Ollama, Google, OpenRouter, Anthropic, Grok), pero no tiene un provider explícito para backend vLLM custom como destino principal para chat/completions y embeddings con configuración clara en backend + UI.

## VALUE
- Permite self-hosting completo del stack de inferencia (LLM + embeddings) sobre infraestructura propia.
- Reduce lock-in con proveedores cloud usando contrato OpenAI-compatible de vLLM.
- Mantiene patrón actual de Archon (provider configurable por settings) con mínimo impacto en servicios existentes.

## ACCEPTANCE CRITERIA
- [x] Se puede seleccionar `vllm` como `LLM_PROVIDER` y como `EMBEDDING_PROVIDER` desde settings.
- [x] `get_llm_client()` crea cliente OpenAI-compatible apuntando a `VLLM_BASE_URL` para chat/completions.
- [x] `create_embeddings_batch()` funciona con vLLM vía `/v1/embeddings` usando modelo configurable.
- [x] Validación de provider/modelo incluye `vllm` en backend y frontend sin romper providers actuales.
- [x] Errores de conexión/timeouts del backend vLLM se reportan con excepciones tipadas y contexto útil.
- [x] `.env.example` y docs reflejan configuración vLLM (URL, key opcional, modelos chat/embedding).
- [x] Flujo backward-compatible: providers existentes siguen operando sin cambios.

---

## TECH CONTEXT

### Bounded Context
- Backend provider routing: `python/src/server/services/llm_provider_service.py`
- Embeddings adapter flow: `python/src/server/services/embeddings/embedding_service.py`
- Provider config source: `python/src/server/services/credential_service.py` (`get_active_provider`)
- Settings UI: `archon-ui-main/src/components/settings/RAGSettings.tsx`

### Integration Target
- vLLM OpenAI-compatible endpoints:
  - `POST /v1/chat/completions`
  - `POST /v1/completions`
  - `POST /v1/embeddings`
- En Archon, la integración se implementa como nuevo provider lógico `vllm` reutilizando cliente OpenAI-compatible.

---

## INVARIANTS

```text
I-1: ∀ req ∈ {chat, embedding}: provider(req) = vllm ⇒ base_url(req) ≠ ∅

I-2: provider = vllm ⇒ (∃ m_chat : m_chat ≠ "") ∧ (∃ m_embed : m_embed ≠ "")

I-3: ∀ p ∈ ExistingProviders: behavior_after(p) = behavior_before(p)

I-4: error(vllm_transport) ⇒ raised_exception ∈ TypedProviderErrors ∧ includes(context)
```

---

## OUT OF SCOPE
- Orquestación/auto-scaling de clúster vLLM (K8s, autoscaler, sharding).
- Fine-tuning, LoRA lifecycle management o training jobs.
- Migración automática de modelos entre proveedores.
